Structure
- NeuralLayer
    - in_size
    - out_size
    - weights


L = l(y), L' = l'(y) * y'
y = f(x)

L = l(f(x))




BackPropogation
- there will be a final model object with a function: model.backward()
    - 


BCE der: 
- -(a*log(x) + (1-a)*log(1-x)) 
= -d (a*log(x) + (1-a)*log(1-x)) / dx
    = d (a*log(x)) / dx + d ((1-a)*log(1-x)) / dx
    = a/x + (1-a)/1-x
    = y_label/y_pred + (1-y_label)/(1-y_pred)
    = 0/0.899 + (1-0)/0.101 = 9.9

sigmoid der:
- 1/(1 + exp(-x))
= d ((1 + exp(-x))^-1) /dx
    = -1*(1 + exp(-x))^-2 * d (1 + exp(-x)) / dx
        = d (1 + exp(-x)) / dx
        = -exp(-x)
    = exp(-x) / (1 + exp(-x))^2


linear layer der:
x = [x0, x1, x2, x3] (4,)
W = [[w0, w1, w2, w3]] (1, 4)
b = [b0, b1, b2, b3] (4,)
n = scalar output

- Wx + b = n
der w respect to x
- d (n) / dx
- = W
der w respect to W
- d (n) / dW
= [
    [d(n)/w0],
    [d(n)/w1],
    [d(n)/w2],
    [d(n)/w3],
]

